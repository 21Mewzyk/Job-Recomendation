{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "from nltk.collocations import *\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CV\n",
    "f=open('textfile.txt','r', errors = 'ignore')\n",
    "text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(x):\n",
    "    word_sent = word_tokenize(x.lower().replace(\"\\n\",\"\"))\n",
    "    _stopwords = set(stopwords.words('english') + list(punctuation)+list(\"●\")+list('–')+list('’'))\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    NLP_Processed_CV = [lemmatizer.lemmatize(word) for word in word_tokenize(\" \".join(word_sent))]\n",
    "#     return \" \".join(NLP_Processed_CV)\n",
    "    return NLP_Processed_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_word(x):\n",
    "    finder = BigramCollocationFinder.from_words(x)\n",
    "    lst = []\n",
    "    Key_word_from_CV = []\n",
    "    for i in sorted(finder.ngram_fd.items()):\n",
    "    # if a double word appears more than once, then print it out.\n",
    "        if i[1] > 1:\n",
    "            print(i)\n",
    "            lst.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # print(\"************************\")\n",
    "    for j in lst:\n",
    "    #     print(\" \".join(j))\n",
    "        Key_word_from_CV.append(\" \".join(j))\n",
    "    return Key_word_from_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_Processed_CV = nlp(text)\n",
    "# NLP_Processed_CV=' '.join(NLP_Processed_CV)\n",
    "NLP_Processed_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = get_key_word(NLP_Processed_CV)\n",
    "key_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing scapped df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DF_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"JobID\"]=df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['All'] = df[df.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['All']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"].replace(r'\\n','', regex=True,inplace=True)\n",
    "df[\"description\"].replace(r'\\r','', regex=True,inplace=True)  \n",
    "df = df.replace(' ', '')\n",
    "df.description = df.description.str.strip()\n",
    "df.All = df.All.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put keywords of the CV into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "# append columns to an empty DataFrame\n",
    "df2['title'] = [\"I\"]\n",
    "df2['job highlights'] = [\"I\"]\n",
    "df2['job description'] = [\"I\"]\n",
    "df2['company overview'] = [\"I\"]\n",
    "df2['industry'] = [\"I\"]\n",
    "\n",
    "# Compare with the key words from CV only\n",
    "df2['All'] = \" \".join(key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#klnsdsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(top, df_all, scores):\n",
    "    recommendation = pd.DataFrame(columns = ['positionName', 'company',\"location\",'JobID','description','score'])\n",
    "    count = 0\n",
    "    for i in top:\n",
    "#         recommendation.at[count, 'ApplicantID'] = u\n",
    "        \n",
    "        recommendation.at[count, 'positionName'] = df['positionName'][i]\n",
    "        recommendation.at[count, 'company'] = df['company'][i]\n",
    "        recommendation.at[count, 'location'] = df['location'][i]\n",
    "    \n",
    "        recommendation.at[count, 'JobID'] = df.index[i]\n",
    "        recommendation.at[count, 'description'] = df['description'][i]\n",
    "        recommendation.at[count, 'score'] =  scores[count]\n",
    "        count += 1\n",
    "    return recommendation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine_similarity + TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def TFIDF(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # TF-IDF Scraped data\n",
    "    tfidf_jobid = tfidf_vectorizer.fit_transform(scraped_data)\n",
    "\n",
    "    # TF-IDF CV\n",
    "    user_tfidf = tfidf_vectorizer.transform(cv)\n",
    "\n",
    "    # Using cosine_similarity on (Scraped data) & (CV)\n",
    "    cos_similarity_tfidf = map(lambda x: cosine_similarity(user_tfidf,x),tfidf_jobid)\n",
    "\n",
    "    output2 = list(cos_similarity_tfidf)\n",
    "    return output2\n",
    "    \n",
    "output2 = TFIDF(df['All'], df2['All'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = sorted(range(len(output2)), key=lambda i: output2[i], reverse=True)[:100]\n",
    "list_scores = [output2[i][0][0] for i in top]\n",
    "TF=get_recommendation(top,df, list_scores)\n",
    "TF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine_similarity + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_jobid = count_vectorizer.fit_transform(df['All']) #converting job data into vectors using count vectorizers\n",
    "count_jobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count_jobid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = count_vectorizer.transform(df2['All'])#converting user cv data into vectors using count vectorizers\n",
    "user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_countv = map(lambda x: cosine_similarity(user_count, x),count_jobid)\n",
    "output3 = list(cos_similarity_countv)\n",
    "#output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = sorted(range(len(output3)), key=lambda i: output3[i], reverse=True)[:100]\n",
    "list_scores = [output3[i][0][0] for i in top]\n",
    "cv=get_recommendation(top, df, list_scores)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.description[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer + NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def KNN(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    \n",
    "    KNN = NearestNeighbors(n_neighbors=100,p=2)\n",
    "    KNN.fit(tfidf_vectorizer.fit_transform(scraped_data))\n",
    "#     NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv), return_distance=True)\n",
    "    NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv))\n",
    "    top = NNs[1][0][1:]\n",
    "    index_score = NNs[0][0][1:]\n",
    "\n",
    "    knn = get_recommendation(top, df, index_score)\n",
    "    return knn\n",
    "\n",
    "knn = KNN(df['All'], df2['All'])\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn.to_csv('knn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined TFIDF, CV and KNN result together, to make a dataframe \"final\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = knn[['JobID','positionName', 'score']].merge(TF[['JobID','score']], on= \"JobID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge1 = knn[['JobID','title', 'score']].merge(TF[['JobID','score']], on= \"JobID\")\n",
    "# final = merge1.merge(cv[['JobID','score']], on = \"JobID\")\n",
    "# final = final.rename(columns={\"score_x\": \"KNN\", \"score_y\": \"TF-IDF\",\"score\": \"CV\"})\n",
    "# final.head()a\n",
    "\n",
    "final = merge1.merge(cv[['JobID','score']], on = \"JobID\")\n",
    "final = final.rename(columns={\"score_x\": \"KNN\", \"score_y\": \"TF-IDF\",\"score\": \"CV\"})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale it\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "slr = MinMaxScaler()\n",
    "final[[\"KNN\", \"TF-IDF\", 'CV']] = slr.fit_transform(final[[\"KNN\", \"TF-IDF\", 'CV']])\n",
    "\n",
    "# Multiply by weights\n",
    "final['KNN'] = (1-final['KNN'])/3\n",
    "final['TF-IDF'] = final['TF-IDF']/3\n",
    "final['CV'] = final['CV']/3\n",
    "final['Final'] = final['KNN']+final['TF-IDF']+final['CV']\n",
    "final.sort_values(by=\"Final\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv('final.csv', index=False)\n",
    "final2 = final.sort_values(by=\"Final\", ascending=False).copy()\n",
    "final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final2.merge(df, on=\"JobID\")\n",
    "final_df = df.merge(final2, on=\"JobID\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"final_df.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d86ab636b363cb32d3f7578512f6396b4a169d210dbf8b4d547e88bd9fd74388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
